\documentclass[11pt]{article}
\usepackage{../inc/style}
\title{Morgol specifications}
\author{\href{mailto:thierry.coppey@epfl.ch}{\color{black} Thierry Coppey},
	\href{mailto:andres.notzli@epfl.ch}{\color{black} Andres Noetzli}}

\newlength{\dlen}
\def\discuss#1{\par\hspace{2em}
\setlength{\dlen}{\textwidth}
\addtolength{\dlen}{-2em}
%\addtolength{\dlen}{-\leftmargin}
\begin{minipage}{\dlen}\footnotesize {\bf\color{red} Discussion:} #1\end{minipage}\par}
\def\say#1#2{\begingroup\par\leftskip1em {\bf #1:} \it #2\par\endgroup}

\newcommand{\lift}{\text{^=}}

\begin{document}
\tableofcontents
% --------------------------------------------------------------------------------------------------------------
\newpage
\section{Language syntax and semantics}
\discuss{The goal of this external DSL is to be syntactically close from SQL (whereas Scala internal DSL will be more concise), in particular: case insensitivity, comments style and keywords verbosity.}
Let {\tt ident} an identifier string, {\tt longLit}, {\tt doubleLit} and {\tt stringLit} (quoted string) literals and ${\tt type}\in\{{\tt long}, {\tt double}, {\tt string}, {\tt date}\}$ a type declaration. Additional types are converted into these during the parsing.
\discuss{What about user-defined types? Targeting Scala and C/C++, we should restrict to case classes/C {\tt struct}, but these could easily be flattened into multiple values in several columns. Benefit of arbitrary types is not clear.}
For conciseness, we denote: \verb$rs(a,b) := (a (b a)*)?$ and \verb$rs1(a,b) := a (b a)*$. Comments are delimited by: {\tt ----...\textbackslash n} , {\tt //...\textbackslash n} or {\tt /*...*/}.\\
The system declaration file format is defined as:
\begin{verbatim}
system ::= source* mapdef* trigger*
\end{verbatim}

% --------------------------------------------------------------------------------------------------------------
\subsection{Sources}
\begin{verbatim}
source ::= "CREATE" ("STREAM" | "TABLE") schema "FROM" input split adaptor ";"
schema ::= name "(" rs1(field type, ",") ")"
input  ::= "FILE" quotedPath
split  ::= ("LINE" | quotedSeparator) "DELIMITED"
         | "FIXEDWIDTH" sizeInBytes
         | "PREFIX" headerBytes
adaptor ::= name "(" rs(option ":=" value, ",") ")"
\end{verbatim}

\discuss{Andres suggests that input should only be defined by an inteface that the programmer implements (we provide some built-ins: file, network, ...) to support arbitrary sources. Flexibility is important to accomodate with existing streams. One specialized stream would be a clock (see clock join in \hyperref[pb:pagerank]{PageRank}).
\say{Thierry}{While I agree with Andres, is it most beneficial to have this also in the external DSL or only available in Scala internal DSL?}
}
\discuss{Do we want to have the converse (output streams)? Are they handled in user-defined triggers, with an interface or through user-defined (Scala) functions (leveraging the apply expression)?}

Source is either input stream (content varies while the program is running and generate events) or fixed tables (constant relations). The implemented adaptors and their respective options are:\ul
\item {\tt ORDERBOOK}: finance events in CSV, with the columns: {\tt transaction\_id}:int, {\tt broker\_id}:int, {\tt evt\_type}:string$\in\{B,S,E,D\}$ (place bid, place ask, match, cancelled), {\tt volume}:int, {\tt price}:double.\ul
	\item {\tt brokers}: number of brokers (integer)
	\item {\tt bids}, {\tt asks}: stream names as stringLit
	\item {\tt deterministic}: boolean defining whether broker id is assigned randomly
	\ule
\item {\tt CSV}\ul
	\item {\tt name}: string, schema name
	\item {\tt schema}: stringLit of comma-separated types
	\item {\tt delimiter}: column delimiter as string, by default a comma
	\item {\tt action} $\in\{\text{insert, delete, both}\}$. If both, first column denotes the transaction\_id, the second is $\in\{0,1\}$ and is 1 if it is an insertion, following columns are the tuple.
	\ule
\ule

% --------------------------------------------------------------------------------------------------------------
\subsection{Expressions}
\begin{verbatim}
expr   ::= "(" expr ")" | ...
mapref ::= name "[" rs(key, ",") "]"
add    ::= expr "+" expr
mul    ::= expr "*" expr
aggsum ::= "SUM" "(" ("[" rs(key, ",") "]" ",")? expr ")"
exists ::= "EXISTS" "(" expr ")" | "{" expr "}"
lift   ::= "(" ident "^=" expr ")"
cmp    ::= expr ("="|"=="|"!="|"<>"|">"|"<"|">="|"<=") expr
apply  ::= ident "(" rs(expr, ",") ")" // type inferred from library
ref    ::= ident
const  ::= longLit | doubleLit | stringLit
\end{verbatim}
Minor changes simplify the M3 language, otherwise semantics remains identical.
Table access are replaced by map references because they share the same semantics. 
We have aliases for exists (conciseness) and two comparison operators (compatibility).
AggSum with no key is equivalent to empty key.
We need to lookup built-in functions return type (and possibly accepted arguments).
\discuss{For function lookup, how to avoid declaration duplication (Bytecode inspection)?}
\discuss{Simplify grammar by also using {\tt =} for lift? This introduce ambiguity between lift and cmp as they are differentiated only by the context (depending if the lhs ref is bound), but after all, comparison of two bound variable is semantically equivalent to lift one as the other.}

\discuss{Do we need explicit AggMin and AggMax operations or are these handled by maps?}
% aggmin ::= "Min" "(" ("[" rs(key, ",") "]" ",")? expr ")"
% aggmax ::= "Max" "(" ("[" rs(key, ",") "]" ",")? expr ")"

% --------------------------------------------------------------------------------------------------------------
\subsection{Maps (internal structures)}
A table has an associated map in which the table will be loaded. A table does not generate trigger whereas there are always triggers associated with a stream. The default type of a map is {\tt long} if not specified. A map associated with an input stream is implicitly created if the stream is accessed by a mapref (lookup/foreach). The output keywords has exactly the same semantics as declaring the map being result of a query in M3.
% In Andres original proposal it was PUBLIC instead of OUTPUT. Good idea :)

\begin{verbatim}
mapdef ::= "DECLARE" "OUTPUT"? "MAP" name "[" rs(key, ",") "]" ("[" rs(aggkey, ",") "]")? 
                                 (":" type)? (":=" expr)? mapopt? ";" 
key    ::= ident ":" type
aggkey ::= "filter"? ("min"|"max"|"sum") "(" ident ")" ":" type
mapopt ::= "WITH" "(" rs(option ":=" value, ",") ")"
\end{verbatim}
\discuss{Are all type annotations necessary?
	\say{Andres}{Aggregation keys (and possibly keys) types should be inferred}
	\say{Thierry}{Agree, but not sure we can always derive the correct type because we might have ordering issues and it gets harder with recursion. I prefer them mandatory now and make them optional later. Also note that the aggregation does not need to happen on a key but any bound variable in rhs.}}

If the expression is not defined, the map is user-managed and the programmer needs to write his own triggers to maintain it (see Section~\ref{section:triggers} for details).

{\bf Semantics}
\discuss{most suble topic as we want to have a simple syntax for 3 concerns:\\
-- Number of shortest paths between two vertices (similar as \cite{socialite} running example)\\
-- Simple grouping with aggregations as in TPC-H query 1\\
-- Range conditions as in AXFinder financial query\\
and blend this nicely with incremental domain maintenance in the implementation.}

First key part is the grouping columns, second part is aggregation values for the group, multiplicity is the number of tuples in the group. Aggregation values semantics is <<element>> for write and <<aggregate>> for read. Finally, if a <<filter>> keyword precedes the aggregation key, tuples that do not have the same aggregation value will be discarded.

The equivalent SQL query of a filtered aggregation key $k$ is a nested query:
\begin{lstlisting}[language=sql]
SELECT keys AS keys_out, aggrs FROM map
WHERE k = (SELECT agg_k(k) FROM map WHERE keys=keys_out)
GROUP BY keys
\end{lstlisting}
\discuss{Do we maintain tuples that have been filtered out ? If yes, how does it combine with incremental domain maintenance? How do we distinguish tuples that we need to re-compute? How to maintain correct multiplicity?
\say{Milos}{disagree with this semantics, think that the resulting multiplicity of aggregation should be 1 to keep coherency with the SQL.}
\say{Thiery}{SQL multiplicity of 1 is only for displaying purposes. We retain more power by having the number of elements aggregated to compute average and count easily afterward.}
}

{\bf Map options:}\\
The options that can be declared for a map are the location and the value threshold:
\ul\item The location is defined by one or multiple tags (logical group name) for the map. Nodes join the cluster with one or more tags: they accept to be in charge of a partition of all maps attributed to the tag.
	Moreover, we can tune manually the locality with hashing function.
\ule
\discuss{What if the number of nodes is variable, in particular what if nodes are leaving/joining  (failure/recovery) the system? There are two approaches: static hashing with redundancy (that is read/write from multiple nodes or \href{http://lpd.epfl.ch/alistarh/DistAlgoFall08/da08-GroupMembershipVSC.pdf}{group membership}. Is there an existing framework we can build on?}
\ul\item The threshold is the relative difference in keys/values for a tuple to be considered as an update of the map for recursion. This only applies to double types.
\ule
\discuss{Not clear to which field the threshold relates. Do we need other options ?}
\ul\item Andres suggests that we should have options to hint the compiler about which structure we want to maintain secondary indices (tree, hash) in; to address inequality joins for example. When automatic inference  detects sufficiently well these cases, we can remove this option.
\ule

% --------------------------------------------------------------------------------------------------------------
\subsection{Triggers}
\label{section:triggers}
\begin{verbatim}
trigger ::= "ON" ("+"|"-") streamName "(" ident ("," ident)* ")" "{" (stmt)* "}"
          | "ON" "SYSTEM" "READY" "{" (stmt)* "}"
stmt    ::= mapref (":" "(" expr ")")? ("+="|":=") expr ";"
          | "CALL" ("+"|"-") streamName "(" rs(expr, ",") ")"
\end{verbatim}
The triggers being defined by the user are called before any other trigger. It is not possible to override an automatically generated trigger.
\discuss{When do we call user-defined triggers (before all automatic triggers)?}
\discuss{Are user-defined triggers really need? Have they more flexibility/power than expressions?
\say{Thierry}{To encode specifically clock/window we need to make non-symetric add and deletion triggers.}
}
% --------------------------------------------------------------------------------------------------------------
\newpage
\section{Examples}
\subsection{Betweenness centrality}
Let our stream be $edge(x,y,l)$ (with implicit map). To compute the \href{http://en.wikipedia.org/wiki/Betweenness_centrality}{betweenness centrality} $g(v)$ of graph nodes $v$, we apply the following rules:
\[\begin{array}{rcl}
edge[x,y,l] &:=& \text{\it implicitly, corresponding to input stream} \\
spath[x,y][{\rm filter} \min(l)] &:=& edge[x,y,l] +  edge[x,z,l_1] \times spath[z,y,l_2] \times (l\hat= l_1+l_2)\\
spath_v[x,y,v,l] &:=& spath[x,v,l_1] \times spath[v,y,l_2] \times (l\hat= l_1+l_2)\\
centrality[v] &:=& \exists(spath[x,y,l]) \times (spath_v[x,y,v,l] / spath[x,y,l])
\end{array}\]

% --------------------------------------------------------------------------------------------------------------
\subsection{PageRank} \label{pb:pagerank}
The iterative method of \href{http://en.wikipedia.org/wiki/PageRank#Iterative}{PageRank} is defined for $N$ pages with a matrix $M=(K^{-1} A)^T$ where $A$ is the adjacency matrix of the graph and $K$ the diagonal matrix with out-degrees in diagonal. We synchronously compute (usually $d=0.85$):
\[R(t+1)=d\cdot M\cdot R(t)+\dfrac{1-d}{N}{\bf 1} \qquad \text{with {\bf 1} a column vector of 1s and }R(0)={\bf 1}\cdot \frac{1}{n}\]

Computing this formula asynchronously does not affect correctness but convergence speed (as computation wavefront might visit loops more frequently than necessary).

\[\begin{array}{rrl}
edge[x,y] &:=& \text{\it input stream, adjacency list} \\
node[x] &:=& \text{\it input stream} \in\{0,1\} \text{ or }\exists(edge[x,y] + edge[z,x]) \\
out[x] &:=& {\rm AggSum}([],edge[x,y]) \\
N &:=& {\rm AggSum}([],node[x]) \\
weight[x] &:=& node[x] \times {\rm AggSum}([], \dfrac{edge[y,x] \times weight[y]}{out[y]}) \times d + \dfrac{1-d}{N}\\
&& \text{with }(threshold := 10^{-5})
\end{array}\]

\discuss{To execute statements synchronously, we might have multiple options:\ul
\item {\bf Clock join:} associate a timestamp attribute with all map and create a clock stream. When a clock tick is inserted, the next iteration is computed (join $data \times tick$); when the tick is removed, the associated data can be removed from the map. Issues: deletion trigger of weight must be \underline{overridden} not to be recursive. How do we check that all elements of a tick have been computed ?
\say{Andres}{The nice thing with custom input streams is that you can define the clock stream based on the number of iterations, on a timeout or any other external event.}
\say{Thierry}{See \ref{windowed} for a more detailed solution to create a window of in-flight computations.}
\item \textbf{Limited iterations:} To execute statements synchronously, we could compute only a fixed number of steps. We can then make that number of step vary to reach threshold requirements.
\ule}

% --------------------------------------------------------------------------------------------------------------
\subsection{Optimizations of existing queries}
We want to demonstrate how existing queries could benefit of the language extension.\\
\textbf{TPCH-1:} The \href{http://www.tpc.org/tpch/spec/tpch2.16.0.pdf}{query 1} is a simple aggregation over a single table.
\begin{lstlisting}[language=sql]
SELECT returnflag, linestatus, 
  SUM(quantity) AS sum_qty,
  SUM(extendedprice) AS sum_base_price,
  SUM(extendedprice * (1-discount)) AS sum_disc_price,
  SUM(extendedprice * (1-discount)*(1+tax)) AS sum_charge,
  AVG(quantity) AS avg_qty,
  AVG(extendedprice) AS avg_price,
  AVG(discount) AS avg_disc,
  COUNT(*) AS count_order
FROM lineitem WHERE shipdate <= DATE('1997-09-01')
GROUP BY returnflag, linestatus;
\end{lstlisting}
If we can detect that only aggregation is done over {\it lineitem}, (formally $\Delta Q1a=f(\Delta lineitem)$), we can eliminate the associated map. The query can be rewritten as:
\[\begin{array}{l}
\text{CREATE STREAM }lineitem(quantity,extendedprice,discount,tax,shipdate)\text{ FROM }...\vspace{4pt}\\
\text{DECLARE MAP }Q1A[r,s][sum(q),sum(ep),sum(dp),sum(c),sum(d)] := lineitem[q,ep,d,t,sd] \times\\
\qquad (dp \hat= ep\times(1-d)) \times (c \hat= ep\times(1-d)\times(1+t)) \times (sd < \text{date("1997-09-01")})\vspace{4pt}\\
\text{DECLARE OUTPUT MAP }Q1[r,s,sq,sep,sdp,sc,aq,aep,ad,n] := \\
	\qquad (n \hat= Q1A[r,s][sq,sep,sdp,sc,sd]) \times (aq \hat= sq/n) \times (aep \hat= sep/n) \times (ad \hat= sd/n)\vspace{4pt}\\
\end{array}\]

\textbf{AXFinder:} financial query with range conditions. We examine low-level implementation.
\begin{verbatim}
let mAXFinder = Map[broker_id] -> value
let mAsks = Map[broker_id, price][volume] -> multiplicity
let mBids = Map[broker_id, price][volume] -> multiplicity

on + Bids(b_id, b_vol, b_price) { -- other triggers are very similar
   mAXFinder.add(b_id, 
     ymBids.slice(0,b_id).filter(1,">",b_price+1000).aggr {(k,v) =>  k._3 + v * -b_vol } +
     mBids.slice(0,b_id).filter(1,"<",b_price-1000).aggr {(k,v) =>  k._3 + v * -b_vol }
   )
   mAsks.add((b_id,b_price),(b_vol),1)
}
\end{verbatim}
\discuss{In this example, we store secondary indices in a tree to leverage inequalities. M3 prohibits re-slicing (slicing a slice) because it grows to an exponential number of secondary indices. Assuming we allow only a specific order (broker\_id hash, price tree), the tree structure does not alleviate the iteration because result depends on {\tt b\_id} and a combination of keys/value. Although it remains possible to manually maintain special pre-aggregated values at each tree nodes to avoid iterating, it is tricky to generate such code correctly, and in this particular case, the benefits of tree-structured secondary indices are not clear.}

%\begin{lstlisting}[language=sql]
%SELECT b.broker_id, SUM(a.volume + (-1 * b.volume)) AS axfinder
%FROM   bids b, asks a
%WHERE  b.broker_id = a.broker_id
%  AND  ( (a.price + ((-1) * b.price) > 1000) OR
%         (b.price + ((-1) * a.price) > 1000) )
%GROUP BY b.broker_id;
%\end{lstlisting}

% --------------------------------------------------------------------------------------------------------------
\subsection{Hubs and authorities}
The \href{http://en.wikipedia.org/wiki/HITS_algorithm}{HITS algorithm} is a precursor of the PageRank algorithm.
The following program corresponds to the converging version of the algorithm described on the Wikipedia page.
\[
\begin{array}{rcl}
p[id] &:=& \text{input stream} \\
link[from,to] &:=& \text{input stream} \\
auth_n[id] &:=& auth[id] + AggSum([id], link[id_2,id] \times hub[id_2]) \\
auth\_norm &:=& sqrt(AggSum([], (auth_n[id])^2)) \\
auth[id] &:=& auth_n[id] / norm \quad\text{with} (threshold := X)\\ 
hub_n[id] &:=& hub[id] + AggSum([id], link[id,id_2] \times auth[id_2]) \\
hub\_norm &:=& sqrt(AggSum([], (hub_n[id])^2)) \\
hub[id] &:=& hub_n[id] / norm
\end{array}
\]
\discuss{What's an elegant way of seeding $auth[id]$ and $hub[id]$ with 1 for every $id$?
\say{Thierry}{Something like $hub[id] := p[id] \times ((\exists(hub[id])!=0) + hub_n[id] / norm)$?}
} 

% --------------------------------------------------------------------------------------------------------------
\subsection{Link predictions}
Various link prediction algorithms are described in \cite{linkpred}: ${\bf score}(x,y)$ denotes the probability that a link will be created between nodes $x$ and $y$. Let $\Gamma(v)$ the set of neighbors of a vertex $v$ and $s[x,y]$ the stream of undirected edges between $x$ and $y$. For conciseness, we define $edge[x,y] := s[x,y]+s[y,x]$ such that edges do not need to be duplicated in input stream. If we do not allow multigraphs or weighted edges, we can remove existential quantifiers. We describe the scoring functions as follows:

{\bf Common neighbors:} ${\rm score}(x,y)=|\Gamma(x) \cap \Gamma(y)|$
\[{\rm score}[x,y] := {\rm AggSum}([x,y], \exists(edge[x,z] \times edge[z,y]))\]

{\bf Jaccard's coefficient:} ${\rm score}(x,y)=\dfrac{|\Gamma(x) \cap \Gamma(y)|}{|\Gamma(x) \cup \Gamma(y)|}$
\[{\rm score}[x,y] := \dfrac{{\rm AggSum}([x,y], \exists(edge[x,z] \times edge[z,y]))}
					{{\rm AggSum}([x,y], \exists(edge[x,z] + edge[z,y])}\]

{\bf Adamic/Adar:} ${\rm score}(x,y)=\sum_{z\in \Gamma(x) \cap \Gamma(y)}\frac{1}{\log|\Gamma(z)|}$
\[{\rm score}[x,y] := {\rm AggSum}\left([x,y], \exists(edge[x,z]\times edge[z,y])  \times \dfrac{1}{\log(\exists(edge[z,u]))}\right)\]

{\bf Preferential attachement:} ${\rm score}(x,y)=|\Gamma(x)| \cdot |\Gamma(y)|$
\[{\rm score}[x,y] := {\rm AggSum}([x,y], edge[x,u] \times edge[v,y])\]

{\bf Katz${}_{\beta}$:} ${\rm score}(x,y)=\sum_{\ell=1}^{\infty} \beta^\ell \cdot |{\rm paths}_{x,y}^{\langle\ell\rangle}|$ \\where ${\rm paths}_{x,y}^{\langle\ell\rangle} := \{$paths of length exactly $\ell$ from $x$ to $y\}$\\
%\ul\item Unweighted: ${\rm paths}_{x,y}^{\langle 1\rangle} := 1$ if and only if $x$ and $y$ collaborate.
%\item Weighted/multigraph: ${\rm paths}_{x,y}^{\langle 1\rangle}:=$ number of collaborations between $x$ and $y$.\ule
As we need to sum towards infinity and as usually $\beta \ll 1$ \cite{linkpred}, it is reasonable to bound $\ell \le N$ to finish the computation.
\[\begin{array}{rcl}
path[x,y,l] &:=& edge[x,y] \times (l\hat=1) +  edge[x,z] \times path[z,y,l_1] \times (l\hat= l_1+1) \times (l_1<N)\\
{\rm score}[x,y] &:=& {\rm AggSum}([x,y], path[x,y,l] \times \beta^l )
\end{array}\]

{\bf SimRank${}_\gamma$:} ${\rm score}(x,y)=\left\{\begin{array}{ll}1&\text{if }x=y\\
	\gamma\cdot\dfrac{\sum_{a\in\Gamma(x)}\sum_{b\in\Gamma(y)} {\rm score}(a,b)}
		{|\Gamma(x)| \cdot |\Gamma(y)|} \end{array}\right.$
\[{\rm score}[x,y] := (x=y) + (x\ne y) \times\gamma\times\dfrac
	{\exists(edge[x,a] \times edge[b,y]) \times {\rm score}(a,b)}
	{{\rm AggSum}([x,y], edge[x,u] \times edge[v,y])}\]

% --------------------------------------------------------------------------------------------------------------
\subsection{Connected components}
\href{http://en.wikipedia.org/wiki/Connected_component_(graph_theory)}{Connected components} can be solved in $O(n)$ using BFS/DFS to identify all set members and process sets sequentially. Unfortunately this algorithm can not be described easily in our framework because of its sequentiality. We propose an alternate algorithm similar to \href{http://en.wikipedia.org/wiki/Disjoint-set_data_structure}{disjoint sets} that defines the set to which the vertex belong by its minimal representative. Let $s[x,y]$ the stream of undirected edges between $x$ and $y$ and $edge[x,y] := s[x,y]+s[y,x]$. We have:
\[set[v] := \min(v, edge[v,x] \times set[x])\]

% --------------------------------------------------------------------------------------------------------------
\subsection{Triangles}
Find all triangles (i.e., cliques of size three) in the graph. Let $s[x,y]$ the stream of undirected edges between $x$ and $y$, $edge[x,y] := s[x,y]+s[y,x]$ and assume a total order between vertices.
\[clique_3[x,y,z] := \underset{\text{find the clique}}{\exists(edge[x,y] \times edge[x,z] \times edge[y,z])}
	\;\;\times\;\; \underset{\text{sort vertices}}{((x< \min(y,z)) \times (y < z))}\]

% --------------------------------------------------------------------------------------------------------------
\subsection{Clustering coefficients}
Let a triplet be a set of 3 connected vertices and a closed triplet a 3-clique (triangle).\ul
\item The global \href{http://en.wikipedia.org/wiki/Clustering_coefficient}{clustering coefficient} $C$ is defined as $C:=\frac{\text{\# closed triplets}}{\text{\# triplets}}$.
\[C := \dfrac{{\rm AggSum}([], \exists(edge[x,y] \times edge[x,z] \times edge[y,z]))}
		{{\rm AggSum}([], \exists(edge[x,y] \times edge[x,z]))}\]
\item The local clustering coefficient of a vertex $v_i$ in the directed graph $G=(V,E)$ is given by
$C_i=\frac{|\{ e_{jk}:v_j,v_k\in N_i, e_{jk} \in E \}|}{k_i(k_i-1)}$ with $N_i=\{v_j:e_{ij} \in E \land e_{ij} \in E\}$ and $k_i = |N_i|$.
\[\begin{array}{rcl}
k[x] &:=& {\rm AggSum}([],edge[x,y] \times edge[y,x]) \\
Ci[x] &:=& \dfrac{
	{\rm AggSum}([], (edge[x,y] \times edge[y,x]) \times (edge[x,z] \times edge[z,x]) \times edge[y,z])
}{k[x] \times (k[x]-1)}
\end{array}\]
\item For undirected graph, the local clustering coefficient $C_i$ is doubled. Let $s[x,y]$ the stream of undirected edges and $edge[x,y] := s[x,y]+s[y,x]$. Similarly we obtain:
\[\begin{array}{rcl}
k[x] &:=& {\rm AggSum}([],edge[x,y]) \\
Ci[x] &:=& {\rm AggSum}([], edge[x,y] \times edge[x,z] \times edge[y,z]) \times \dfrac{2}{k[x] \times (k[x]-1)}
\end{array}\]
\item To express the network average clustering coefficient $\bar C = \dfrac{1}{n}\sum_{i=1}^n C_i $, we need an additional stream $node[x]$ describing node existence:
\[{\rm NACC} := \dfrac{{\rm AggSum}([], Ci[x])}{{\rm AggSum}([], node[x])} \]
\ule

% --------------------------------------------------------------------------------------------------------------
\subsection{Max-flow min-cut}
The goal of this problem is the computation of the maximum flow in a network graph.
The graph is directed and every edge has a capacity assigned.
The maximum flow is the largest flow from a source to a sink that the network allows.
The following algorithm is a variation of the \href{http://en.wikipedia.org/wiki/Edmonds%E2%80%93Karp_algorithm}{Edmondsâ€“Karp algorithm}. 
The high level idea is to find the shortest path with remaining capacity from source to sink at every iteration, remove the flow of this shortest path and iterate until there is no path from source to sink anymore.
\[
\begin{array}{rcl}
source &:=& \text{input} \\
sink &:=& \text{input} \\
edge[x,y] &:=& \text{input} \\
residual[x,y,i] &:=& edge[x,y] \times (i \hat= 0) + \\
&&\qquad (residual[x,y,i_n] - spath_e[x,y,i_n] \times iter\_flow[i_n,f] \times f + \\
&&\qquad spath_e[y,x,i_n,flow] \times iter\_flow[i_n,f] \times f) \times (i \hat= i_n + 1)\\
iter\_flow[i][filter \min(f)] &:=& spath_e[x,y,l,i] \times (f = spath_e[x_2,y_2,l_2,i]) \\
spath_e[x,y,l,f] &:=& spath[source,v_1,l_1,i] \times residual[v_1,v_2,i] \times\\
&&\qquad  spath[v_2,sink,l_2,i] \times (flow[x,y,i\times (l = l_1 + l_2 + 1) \\ 
spath[x,y,i][filter \min(l)] &:=& residual[x,y,i] \times (l = 1) +\\
&& spath[x,z,l_1] \times spath[z,y,l_2] \times (l = l_1 + l_2) \\
max\_flow &:=& AggSum([], spath[iteration, f] \times f)
\end{array}
\]
The maps in this algorithm have the following purposes:
\begin{itemize}
\item $source$: Index of the source node
\item $sink$: Index of the sink node
\item $edge$: A map containing the edges with their capacity as values
\item $residual$: The residual network at iteration $i$ (initialized with the edge capacities)
\item $iter\_flow$: The flow at iteration $i$ on the shortest path from source to sink
\item $spath_e$: All edges that are part of the shortest path from source to sink at iteration $i$
\item $spath$: The shortest paths at iteration $i$
\item $max\_flow$: The max flow of the graph 
\end{itemize}

% --------------------------------------------------------------------------------------------------------------
\subsection{Reliable unicast messaging}
This problem has been presented in the Bloom paper and serves as an example of how Bloom programs can be translated to Morgol programs.
On a high level the following program allows to send messages reliably from $src$ to $dst$.
The idea is that the user provides a stream $pipe\_in$ containing the messages that he would like to have sent and $got\_ack$ provides information about which messages have been received by the destination.
The stream $timer$ is used to retry delivery of unsent messages in regualar intervals.
Messages to be delivered are saved in the map $data\_chan$ and ACKs are received on the stream $ack\_chan$.
\[
\begin{array}{l}
\text{CREATE STREAM } timer(time: long) \text{ FROM } periodic(10) \\
\text{CREATE STREAM } pipe\_in(dst : location, src : location, ident : long, payload : string) \text{ FROM } ... \\
\text{CREATE STREAM } ack\_chan(src : location, dst : location, ident : long) \text{ FROM } channel(dst)\\
\\
\text{DECLARE PUBLIC MAP } data\_chan [dst : location, src : location, ident : long][] := \\
\qquad timer[t] \times send\_buf[dst, src, ident] + pipe\_in[dst, src, ident]\\
\text{WITH } local\_map := true \\
\text{DECLARE PUBLIC MAP } got\_ack [ident : long][] := \\
\qquad ack\_chan[s, d, ident] \times send\_buf[d, s, ident]\\
\text{WITH } local\_map := true \\
\text{DECLARE MAP } send\_buf [dst : location, src : location, ident : long, payload : string][] := \\
\qquad pipe\_in[dst, src, ident, payload] \times (\neg\text{EXISTS}(got\_ack[ident])) \\
\text{WITH } local\_map := true
\end{array}
\]

The basic ideas to translate Bloom programs:
\begin{itemize}
\item Channels are translated to streams that read packets from the network
\item Output interfaces are public maps
\item Tables are maps
\item An instance of the Bloom periodic type is represented by a stream of type periodic
\item When a Bloom collection is defined by multiple statements, we union ($+$) them together
\end{itemize} 

\discuss{How do we declare a map to be local? Do we want to do that explicitely or do we distribute the map such that every node stores stores its local information?}
\discuss{Evaluation order?}
\discuss{What's the best way to represent a send-channel? Do we need output streams? Maybe we could just use our implicit mechanism of distributing data?}

% --------------------------------------------------------------------------------------------------------------
\section{Other programming paradigms}
\subsection{Reactive programming}
In Morgol, we can simply encode reactive programming by the fact that we can define arbitrary triggers (on +/- ...) and that in these triggers, we can call other triggers (with the call statement). Additionally, we could extend the function library such that the user can define arbitrary functions.

\subsection{State machine}
An automaton is defined by $M=(Q,\Sigma,\delta,q_0,F)$ with\\
\begin{tabular}{@{}ll@{\hspace{1cm}}ll}
$Q$ & finite set of states & $q_0\in Q$ & initial state \\
$\Sigma$ & finite set of input letters & $F\subseteq Q$ & set of accepting states \\
$\delta:Q\times\Sigma\to Q$ & the transition function \\
\end{tabular}

We can find a correspondence by replacing $\Sigma$ by the set of events received, defining a mapping $Q\to\mathbb{N}$ to encode the state and create a table $T(state,message,next\_state)$ to define $\delta$. $q_0$ can be encoded in the <<on system initialized>> trigger and finally, to generate $evt$ when $q_i\in F$, it suffice to compute $\exists(F[q_i]) \times (evt)$ in the input stream trigger.

% --------------------------------------------------------------------------------------------------------------
\subsection{Windowed streams} \label{windowed}
There is three at least three possible ways of windowing streams:\ul
\item \textbf{Disjoint windows:} can be implemented by emptying all maps periodically or based on an event. An appropriate trigger could be written for such task.
\item \textbf{Fixed-size window:} (size-based sliding) the window contains a fixed number of tuples. Add at stream entrance (master) a LRU log of events (like \href{http://docs.oracle.com/javase/7/docs/api/java/util/LinkedHashSet.html}{{\tt LinkedHashSet}} allowing multiple elements, or a queue with O(1) deletions) where we can retrieve oldest stored elements.\ul
	\item At insertion event, enqueues tuple in the LRU structure, then call triggers
	\item At deletion event, also deletes tuple from the LRU structure, then call triggers
	\item After insertion, while LRU size is larger than window size, delete the oldest LRU element and call deletion trigger as it was an event generated by the data stream.\ule
\item \textbf{Rolling window:} (event-based sliding) can be seen as the rolling union of multiple smaller disjoint windows. Whenever a tuple arrives, it is added with current (max) logical timestamp. When a timeout occurs, related elements are removed from the window.
Let $s[v_1,..,v_n]$ the windowed input stream and $clock[t]$ (with map $clk[t]$) the clock stream that determines the windowed stream $w[v_1,...,v_n,t]$. We have the following triggers: 
\[\begin{array}{l}
{\rm on} + s(v_1,...,v_n) : w.{\rm add}((v_1,...,v_n,c), (c \hat= {\rm ArgMax}([],cl[t]) ))\\
{\rm on} - s(v_1,...,v_n) : w.{\rm add}((v_1,...,v_n,m), -1 \times (m \hat= {\rm ArgMin}([],\exists(w[v_1,...,v_n,t]) \times t)) )\\
{\rm on} + clock(t) : cl.{\rm set}(t,1)\\
{\rm on} - clock(t) : cl.{\rm set}(t,0); w.{\rm set}((v_1,...,v_n,t), 0) \text{ // drop all events at }t\\
\end{array}\]
Note: the clock stream allows both control over window granularity and size.
\ule
\discuss{Do we want to generalize the windowing pattern and make it a built-in function of our language? How does it blend with iterative computations?}

% --------------------------------------------------------------------------------------------------------------
\subsection{Iterative computations (and unstratified programs)}
One frequent pattern arising from the examples above is iterative computation of sets (PageRank, Max-flow). Also Bloom solution to express unstratified programs relies on iterative sets of predicates and negations. In a distributed settings we would like to have non-blocking iterations (multiple in-flight iterations) while at the same time reducing memory consumption by pruning useless intermediate results. The iterative computation poses 3 problems:\ul
\item Throttling: controlling the maximal number or the speed of iterations made?
\item Termination detection: how to detect that a particular iteration is completely finished?
\item Removal: how to blend iterative formula and avoid recursive removal when pruning useless intermediate results?
\ule

We could use a pattern similar to that of windowing stream to address the throttling issue. For removal however, we need to make sure that the removal of an iteration does not call recursive deletion. We might do that with a special option for the iterative map (like {\tt ignore\_recursion(delete,clock)}), which would prevent creating deletion triggers for the $clock$ stream in the specified map.

Finally, detecting that an iteration step $i$ is terminated can be detected by:\ol
\item Locally: inspecting the <<to be called>> messages queue and detected locally that all the messages to be processed are referring to a later iteration.
\item Remotely: ensure all messages of that step have been processed (have a step-specific distributed acknowledgement counter?) (see \ref{distrib}).
\ole

% --------------------------------------------------------------------------------------------------------------
\section{Implementation}
\subsection{Distribution model} \label{distrib}
A single \textit{master} node receives all input streams. Triggers execution is then parallelized over \textit{workers} with asynchronous messages between master and workers and between workers. Maps are partitioned between workers such that it is possible to compute (hashing) the worker responsible of a key. There are 3 types of messages being exchanged (denoting operations on a map):\ul
\item \textbf{Call:} remote procedure invocation, does not return any value: {\tt add}, {\tt set}, {\tt foreach}, {\tt clear}
\item \textbf{Request:} {\tt get} (lookup), {\tt aggregate} (foreach with return value), {\tt collect} (map snapshot)
\item \textbf{Response:} answer to a request
\ule

Statements are executed sequentially on the master. Whenever the master detects that there is a dependency (RaW or WaW with {\tt set} and {\tt clear}), it makes sure that all previous data has been flushed into workers' map with a barrier procedure. The worker are purely reactive: they process messages one by one in the order of arrival. Requests block until response on the master, whereas they are non-blocking on workers.

\subsection{Barrier}
The goal of the barrier is to synchronize all workers, ensuring that all messages sent have been received and processed. We distinguish two cases:\ul
\item Request: acknowledgment is implicit in the response. Each worker ensures that it has no pending request before starting/continuing the barrier procedure.
\item Call: workers and master maintain locally the number of remote calls started remotely on each worker in a map $C_S\langle worker\to\#calls \rangle$. Additionally, workers maintain locally the number of calls $C_R$ they have received.
\ule

\subsubsection*{Barrier procedure}\ol
\item Master broadcasts a \textit{barrier-start} message to enable the barrier mode of the workers. At this point, the master is not allowed to send additional messages (except responses).
\item In barrier mode, after a worker has processed a message, if it has no pending request:\ul
	\item If counters are non-zero ($C_R\ne 0 \lor \exists w. C_S[w]\ne 0$), send their values $\langle C_R, C_S\rangle$ to the master and reset them to zero.\ule
\item At each message reception, the master computes the difference of calls sent and received for each worker ($\sum_{w}C_S[w] - w.C_R$).
\item When all counters are zero (there is no in-flight message), the master broadcasts a \textit{barrier-stop} message and continues execution.
\ole

\subsubsection*{Correctness and termination proofs}
We need to prove that the barrier procedure terminates, and at the end, all message sent have been received.
We consider the settings of reliable channels (exactly once delivery) but no FIFO guarantee (such that we could use UDP with delivery guarantee instead of TCP if needed). We do not consider failures here, otherwise termination property will (obviously) not be satisfied.

First notice that we can ignore requests/response in the proof:\ul
\item If the request is issued by master, response must be received before barrier (blocking).
\item Workers do not create request spontaneously, they result from a call.
\item If an incoming call create a request on a worker, the delivery of acknowledgement will be delayed until the request is responded (step 2), thus a request can only happen when not all calls have been acknowledged, hence barrier cannot complete before all requests have been answered.
\item Because of channel delivery guarantee, eventually, all requests will be answered.
\ule

We prove correctness and termination by induction:\ul
\item Case 1: consider 1 master, 1 worker; the master has sent $n\ge 0$ calls to the worker. At step 2, worker has $k\ge 0$ calls processed, and $r\ge 0$ pending requests. Eventually, all requests will be answered and all messages will arrive (reliable channel) and be processed. Either of these events will generate an acknowledgment to the master (step 2). Because each message is counted exactly once (counter reset), the master will receive exactly $n$ acknowledgements. 

\item Case 2: consider 2 workers; assume worker $w_1$ receive a call whose processing generates $n\ge 0$ calls from $w_1$ on $w_2$. We have the following cases:\ul
	\item If $n=0$ we are in base case 1.
	\item $w_1$ finishes processing and send $\langle1,w_2\to n\rangle$ to master.
	Because it atomically acknowledge his call and create $n$ calls for $w_2$, it is similar as if master would have sent $n$ messages to $w_2$ (case 1).
	\item $w_2$ acknowledge master before $w_1$, master needs $-n \text{ acks} =n \text{ calls}$ for $w_2$, which will be compensated when receiving $w_1$ message.
	\ule
\item Induction: we can reduce any complex calling chain to the case 1 by iteratively using the case 2 reduction, thus reducing the call chain by 1. Finally, we are left with 1 master and $N$ independent worker. Since workers are independent, it reduces to $N$ times the case 1 \#
\ule

Termination proof: whenever a node create a remote call, it counts it positively, whenever a node receives a call, it is counted negatively. Assuming that message processing terminates, all messages will be counted once positively and once negatively, so eventually all counters will be zero and the barrier will terminate.

\subsection{Dependency detection}
Intuition: mark as \textit{invalid} maps that might be affected by an in-flight (unacknowledged) messages. Maps read need to be valid. As {\tt set} is not commutative, the destination map must be valid (not to be affected by previous messages). As {\tt add} is commutative, it is not necessary to force the destination to be valid.
% - A write invalidates the map
% - A map needs to be valid to be read from
% - When flushing, all writes are committed to the map

We encode the dependencies detection (to place barriers where appropriate) in two steps:\ol
\item Compilation: each statement is either {\tt add} or {\tt set} on a map, and read from other maps. We prefix all operations by $\langle mi,\{ mv_1,mv_2,...mv_n \}\rangle$ where $mi$ is the map invalidated by the operation and $mv_i$ are the maps that must be valid.
\item Runtime: the master maintains the list of invalid maps. If one of the requested map is invalid (in the prefix), a barrier will be issued, the list of invalid maps is cleared and the operations can take place.
\ole
\discuss{What about recursion? Do all recursive calls go through the master?
	\say{Thierry}{Yes because the trigger for a stream event is known only at the master level, although this sounds like a bottleneck for performance. How could we avoid going through the master?
	In general, we should be able to circumvent the master if the updates are \underline{associative and commutative} (for example {\tt add} operation); this should be described in Datalog/Bloom papers and might possibly be linked with Daniel's work.}
}

\subsection{Distribution strategy (wip)}
The master process input streams and coordinates worker actions; it does not own any map.
\discuss{What about single-valued maps ({\tt K3Var}) ? Do we maintain them at master and broadcast their modification to all workers in a separate pattern/piggybacked on the barrier stop?}
 A worker \textit{owns} a partition of all maps, determined by a hashing function $hash(map,key)\to worker$. We want to determine the hashing function for all maps such that we can improve locality of incremental updates. Ideas: \ul
\item In equi-join between two relations, we want to hash these two relation along the joining key.
\item For arbitrary join (inequality, cross-product, ...), co-location is not relevant, but hashing should balance the load between nodes. To minimize the number of message exchanged, it would be better if iteration is done on the largest relation, while the smaller is remotely accessed ({\tt get}) and its replies are cached.
\ule
\discuss{Other ideas about how to partition data efficiently ?}

{\color{gray}\footnotesize
------------------ legacy discussion\\
\verb$mapdef ::= "DECLARE" "MAP" name "[" rs(key, ",") "]" ("[" rs(aggkey, ",") "]")? $\\
\verb$                                 (":" type)? ":=" expr mapopt? ";" $\\
\verb$key    ::= ident ":" type$\\
\verb$aggkey ::= ("min"|"max"|"sum") "(" ident ")" ":" type$\\
\verb$mapopt ::= "WITH" "(" rs(option ":=" value, ",") ")"$
\discuss{Do we need to mark for inequality comparison internally ? (maintain in tree)}
\discuss{It is not very clear whether aggregation keys should be separated from other keys as they must be provided for addition and deletion, but are read differently.}
\discuss{We need to detect these "modifications" efficiently and compute its delta over the map to create a reaction to it (call other triggers with updates values). It is not clear how a change in an aggregate value will be treated. Should it be a "deletion/insertion" (not desirable because if we have deletion, we have endless loop in shortest path) or a "value update"?
	\say{Thierry}{I think there is many issues in the nitty gritty details of the implementation that we didn't covered so far.}
We still do not achieve the effect that is claimed in SociaLite: having one single map to compute the shortest path \textit{and} computing its multiplicity. What should the return value of a map containing aggregations?
	\say{Thierry}{The returned value should be the multiplicity of the min/max/sum so that we can achieve average and counting easily. We can easily modify it by wrapping into an exists. The shortest path iteration should be done with the number of added tuples.}
	\say{Milos}{Returned value should be 1 when there is aggregted values (1 aggregate tuple). The number of shortest path is computed using two different maps, one for the minimum length, one for the count.}
	\say{Thierry}{If we always return 1, could we then put the aggregated value there?}
}
\discuss{How do these concerns blend with recursion and incremental maintenance?}
\discuss{Can we do more, something that DataLog could not do ? Find other interesting algorithms.}
% Andres: I would suggest to get rid of ON SYSTEM READY and have a stream of system events instead
% ==> reduce tables to only what necessary ? not sure we can always do that.
------------------ legacy discussion end
}

% ------------ LEGACY PROBLEMS --------------
%\subsubsection*{Problem 2: PageRank}
%\textbf{Clock join:} One possible idea to virtually execute the program synchronously is to have a clock stream and a timestamp attribute for all maps. When a clock tick is inserted, the next iteration can be computed (join $data \times tick$); when the tick is removed, the associated data can be removed from the map. Issues:\ul
%\item Deletion trigger of weight must be \underline{overridden} to only remove data (and avoid recursion).
%\item How to know that all the data at a tick has been fully computed ?
%\ule
%\[\begin{array}{rrl}
%tick[t] &:=& \text{\it logical clock} \in \{0,1\} \\
%node[x] &:=& \text{\it input stream} \in\{0,1\} \\
%edge[x,y] &:=& \text{\it input stream} \\
%weight[x,t] &:=& node[x] \ \times {\rm AggSum}([x], \dfrac{edge[y,x] \times weight[y,t-1]}{{\rm AggSum}([],edge(y,z))}) \times d + \dfrac{1-d}{{\rm AggSum}([],node[z])}
%\end{array}\]
% ------------ LEGACY PROBLEMS END --------------

\section{Roadmap}
Target the following domains and concepts: incrementality, continuous queries, active databases/triggers, recursivity, location and distribution (failure tolerance?), windowed streams, state machine, graphs, declarativeness, deltas, stratification.
\subsubsection*{Language design}\ol
\item Provide an clean, simple and easy to understand semantics for each of the above domains.\ul
	\item Stratification: simulate timestamps / iteration numbers? Provide the user facilities like $<\!\!+$, $<\!\!-$ to do operations at the next time step? recursion = non-recursive with fixed \# iterations. orthogonal serie of maps.
	% look at orchestra (datalog + incrementally) [not very interesting]
	% http://webdam.inria.fr/College/090512Abiteboul.pdf
	\item Non-stratified $\to$ stratified standard scheme (user-defined / built-in ?)
	\item Allow reuse of incrementalization infrastructure and combine recursivity and delta. How to connect a map to its delta to use them.
	\item Inflationary semantics? What's cleanest language?
	\ule
\item Find use cases and examples (at least 1 per domain/concept)
\item Describe derivation rules for incrementalization (what makes incrementalization easier?)
\item Describe data structures used in the back-end.\ul
	\item Have column-oriented / nested tables to avoid keys duplication? compatible with secondary indices ?
	\ule
\item Define distribution strategy and data locality specifications. Must be be orthogonal to other concerns (tag with @all, @group, @hashing key part ?). How to handle hashing?
\ole

Integrate features from Socialite\cite{socialite}, WebDamLog\cite{webdamlog}, Bloom lattices\cite{bloom_lattices}, Dedalus\cite{dedalus}, CALM\cite{bloom_calm}, Pregel\cite{pregel}, DatalogFS\cite{datalog_fs} in DBToaster\cite{dbtoaster09,dbtoaster11}.

\subsubsection*{Implementation tasks}\ol
\item Automate function type checking from library (+extension for arbitrary object)
\item Add attributes/options to maps (AST), move map index creation into type check
\item Multiple values/aggregations and filtering for maps, tree secondary indices ? (inequality)
\item Recursion in the rules (call statement)
\item LMS code generator
\item More options for streams input
\item Distribution framework
\item New parser for Morgol language
\item Incrementalization (rules$\to$triggers conversion) by reusing DBToaster front-end
\ole

% Bibliography
\newpage
\bibliographystyle{plain} \addcontentsline{toc}{section}{References} % acm
\def\pdfurl#1{\href{#1}{\footnotesize pdf}}
{\small \bibliography{../inc/bibliography.bib}}
\end{document}
